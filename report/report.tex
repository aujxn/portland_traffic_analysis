\documentclass{article}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\graphicspath{{graphics/}{../traffic_data_analysis/traffic_data_analysis/plots/}}

\pretitle{
  \begin{center}
  \huge
  \includegraphics[width=10cm]{graphics/PSU_logo_transparent.png}\\[\bigskipamount]
}
\posttitle{\end{center}}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{float}
\pagestyle{fancy}
\rhead{\includegraphics[width = .05\textwidth]{graphics/psulogo2.jpg}}
\usepackage{geometry}
\geometry{a4paper, margin=0.75in}

\title{Portland Traffic Analysis: Final Report}
\author{
  Xu, Suru\\
  \texttt{suru@pdx.edu}
  \and
  Nelson, Austen\\
  \texttt{ajn6@pdx.edu}
}

\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section{Executive Summary}

\section{Introduction and Background}

The primary objective of this project is to analyze highway traffic count data to identify changes in traffic patterns pre- and post-pandemic, with a specific focus on the I-5 and Glenn Jackson bridges. The Oregon Department of Transportation (ODOT) seeks a comprehensive, data-driven analysis using historical traffic data from Automatic Traffic Recorders (ATR). The report will address the following key questions:

\begin{itemize}
    \item How have peak traffic periods changed in terms of structure (flatter, sharper, later, or earlier throughout the day) at these key locations?
    \item How has daily traffic demand shifted across different days of the week at these bridges?
    \item Are there distinct trends between these two major highway crossings?
\end{itemize}

The data provided 
\section{Data}

ATR data provides 24/7 traffic volume at specific highway locations across Oregon and has been curated by ODOT’s monitoring unit for high reliability. The data consists of vehicle counts over hourly time periods at 12 different highway locations, each consisting of 2 opposite directions (either NB/SB or EB/WB). Some locations have many more years available than others, but in total there are about 3.25 million hours of data recorded over the 24 location and direction combinations.

Many trends and area specific patterns can be easily identified by simply looking at the median values in the data. Table \ref{tab:medians} presents an approximation to weekday peak-time and peak-volume based on cubic periodic spline interpolation of hourly medians. 

General visible trends seem to be a slightly later peak traffic time in the mornings and location dependent changes in afternoon peak times. The peak volume is lower post-Covid all across the board, with very few exceptions outside of data availability issues. 

\subsection{Data Quality}
Overall the provided dataset is high quality and quite complete. For a visual representation of the time coverage for each location / direction combination, see figure \ref{fig:data_availability}. The only issues with data quality we discovered in our analysis fall into two main categories:

\begin{enumerate}
    \item Daylight Savings (DST) Inconsistencies 

        The raw provided data is given in Coordinated Universal Time (UTC) format, which does not observe any DST seasonal changes. Filtering the data for duplicated hours results gives occasional duplicates at midnight in April (before 2006) and March suggesting that some timestamps were recorded or converted improperly. A visual of one of the examples where this happens is shown in 
        %\ref{fig:dst_example}, 
        where counts from March through November are clearly shifted exactly one hour forward during the early morning hours when the traffic is most consistent. For most of the analysis we discuss this shouldn't pose a huge issue, but table \ref{tab:medians} approximates what time peak rush hour happens based on hourly medians and occasionally shows a drastic near hour shift of the peak time (see Vista Ridge EB 2018, Wilsonville SB 2024, and Interstate Bridge NB 2024). This makes it difficult to make conclusions about shifts in the traffic structure without more careful analysis. Fortunately, the morning traffic for almost every location is extremely consistent and predictable and identifying and correcting these errors should be possible if that is of interest. 

    \item Under-Reported and Malformed Data

        During the initial interview, the client warned that some of the data is lower than it should be as a result of sensor issues. The client provided us a report of which locations and time periods have such issues and specifying that some data was estimated. One example of estimated data is I5 bridge SB for the year 2018. During this time there was a construction project so traffic could potentially be lower, but reported (potentially estimated) values are so low that the period is not useful for analysis. This is immediately clear in the table \ref{tab:medians} for the Interstate Bridge 2018 median entry. This is a primary location and time period of focus for our analysis, so this data is omitted from the models we create. 

        Another example of malformed data is periods of time with many zero values. In the reliability report it is stated that the 2024 Vista Ridge Tunnel data is reliable, but from 10-07 to 12-23 there are 1440 hourly entries with 0 and 1872 non-zero hourly entries.

        One suggestion to address some of these issues would be to add an additional column(s) to the curated data indicating low reliability or estimated entries. This would enable easy filtering when creating models and comparing model results with or without estimated or unreliable data.
\end{enumerate}

A complete listing of potential issues we found in the analysis can be found in appendix \ref{app:data_issues}. 

\section{Methods}

\subsection{Generalized Additive Model (GAM)}

One of the most simple but ubiquitous models in data science is linear regression, but as the name suggests isn't suitable for capturing non-linear relationships in data on its own. A common technique to address this constraint is to choose a suitable non-linear mapping such that the mapped inputs can be modeled with linear regression. GAMs are a class of Generalized Linear Models which provides a framework for selecting such a mapping as the sum over \emph{smooth} functions (often called smoothers). The assumptions for using this kind of model go hand in hand with its additive and smooth properties:
\begin{enumerate}
    \item The target (output) variable \emph{varies smoothly} in response to the explanatory (input) variables. 
    \item The function we are modelling can be decomposed into a sum of functions. 
\end{enumerate}

Isolating the data to a single location / direction and only considering weekday values, the target variable we model is the hourly traffic counts and explanatory variables which this target varies smoothly with are the hour of the day and month of the year. Then we can model the changes in hourly traffic by creating additional interaction terms between hour of day and categorical variables of interest such as before and after Covid or day of the week.

One of the primary reasons to use GAMs is that due to the additive nature the parameters associated with variables of interest give easily interpretable ways to explain dependency effects between variables.
The non-linear smoothers we use are splines. Spline fitting is similar to polynomial fitting, but are generally easier to work with for a variety of reasons (see Runge effect, sklearn example). 

Our GAM model is as follows:
$$f(h, m, p) = s_1(h) + s_2(m) + s_3(ph)$$
where $h$ is the hour of the day, $m$ is the month of the year, and $p$ is $0$ before Covid or $1$ after. The functions $s_i$ are the splines we need to determine. Each $s_i$ can be decomposed into
$$s_i(x) = w_i^T \hat s_i(x)$$
where $\hat s_i$ is the nonlinear mapping of the explanatory variable $x$ onto a chosen spline basis. \verb|scikit-learn| is a popular \verb|Python| machine learning library which has APIs to easily create the $\hat s_i$ mappings from the desired parameterizations (polynomial order and knot locations). Once the $\hat s_i$ are determined, the problem is traditional (regularized) linear regression to solve for each $w_i$ and the resulting $s_i$ give interpretable \emph{partial dependence} functions for their associated variables.

More advanced GAM specific packages such as \verb|R|'s \verb|mgcv| and \verb|gam| or \verb|Python|'s \verb|PyGAM| have optimization algorithms to determine the appropriate parameters for the smoothing functions automatically. If our model was more complex or included more explanatory variables and interactions, it may be necesarry to use such hyper-parameter tuning. After some experimentation we found the periodic cubic splines with knots:
\begin{itemize}
    \item 15 uniform knots for $\hat s_1$ 
    \item 4 uniform knots for $\hat s_2$ 
    \item knots located at $(0, 4, 6, 8, 10, 15, 17, 19, 24)$ for $\hat s_3$
\end{itemize}
generalized well for our data. The knots for $\hat s_3$ were chosen because we are primarily interested in the effect on peak traffic periods, so we choose the knots to be more densely located during these times to give the model more ability to express differences in the interaction while limiting negative effects of over-paramaterizing the model.

\subsection{Maximum Mean Discrepancy (MMD)}

Traditional statistical methods often rely on parametric assumptions, which may fail to capture the complexity and high-dimensional nature of real-world traffic patterns. To address these limitations, Maximum Mean Discrepancy (MMD) provides a robust, non-parametric alternative for detecting shifts in traffic distributions.
MMD is a kernel-based statistical test that quantifies differences between probability distributions without imposing parametric constraints. By embedding distributions in a reproducing kernel Hilbert space (RKHS), MMD enables a flexible two-sample comparison by mapping traffic data into a high-dimensional space and evaluating the divergence in their mean embeddings. This approach is particularly well-suited for assessing shifts in traffic patterns before and after the pandemic, offering a data-driven framework for capturing complex, non-linear changes in distribution.

\subsubsection{Data Structuring Strategy}

To ensure a systematic comparison of pre- and post-pandemic traffic patterns, the dataset is structured into two key dimensions:
\begin{enumerate}
    \item Time segmentation through monthly groups
    \item Traffic behavior segmentation by day-type classification
\end{enumerate}

This structure allows for temporal consistency and prevents confounding effects from seasonal or weekday variations.

\paragraph{Monthly Grouping for Temporal Consistency}

To account for variation in traffic demand across different times of the year, we divide the dataset into five distinct monthly groups instead of using traditional seasonal definitions. This prevents seasonal confounders from affecting our analysis.

\begin{itemize}
    \item \textbf{January–February:} Typically, the lowest traffic counts due to winter weather conditions.
    \item \textbf{March:} Contains Spring break week, influencing travel demand.
    \item \textbf{April–May \& September–October:} Stable school/work schedules, making them strong baseline months for comparison.
    \item \textbf{June–July–August:} Summer months with a different traffic pattern, characterized by fewer school-related trips and more recreational travel.
    \item \textbf{November–December:} Affected by winter conditions and holiday travel, introducing variability in demand.
\end{itemize}

This grouping strategy ensures that traffic trends are analyzed under similar external conditions before and after the pandemic.

\paragraph{Day-Type Classification for Behavioral Segmentation}

Traffic patterns vary based on the day of the week, requiring further classification to capture meaningful behavioral trends. We categorize traffic data into four primary day types:

\begin{itemize}
    \item \textbf{Monday \& Friday:} Often show distinct travel behaviors, with Friday frequently exhibiting early departures and heavier congestion.
    \item \textbf{Tuesday–Wednesday–Thursday (T/W/T):} Assumed to have relatively consistent traffic patterns, grouped together for statistical efficiency.
    \item \textbf{Saturday \& Sunday (Weekends):} Typically show non-commuting, recreational travel behaviors, distinct from weekday traffic trends.
\end{itemize}

By applying this dual-layered structuring approach, we ensure that comparisons between pre- and post-pandemic traffic data are made under logically consistent conditions, minimizing external sources of variation.

\subsubsection{MMD Model for Traffic Pattern Comparison}

After structuring the dataset, we use the Maximum Mean Discrepancy (MMD) statistic to measure shifts in traffic distributions. Given two sets of traffic data:

\[
X = \{x_{i,j} \mid i = 1, \dots, N; j = 1, \dots, 24\}, \quad Y = \{y_{i,j} \mid i = 1, \dots, M; j = 1, \dots, 24\}
\]

where:

\begin{itemize}
    \item $x_{i,j}$ and $y_{i,j}$ are traffic counts at hour $j$ on day $i$ for pre- and post-pandemic periods, respectively.
    \item $X$ and $Y$ represent hourly traffic counts for each day type within a specific monthly group.
\end{itemize}

The MMD statistic is calculated as:

\[
\text{MMD}^2(X, Y) = \mathbb{E}_{x,x'}[k(x, x')] + \mathbb{E}_{y,y'}[k(y, y')] - 2\mathbb{E}_{x,y}[k(x, y)]
\]

where \( k(\cdot, \cdot) \) is a Gaussian kernel function. This provides a non-parametric measure of distributional shift in traffic behavior.

\subsubsection{Statistical Inference Using Permutation Testing}

Statistical significance is evaluated using a permutation test. Traffic data is pooled and randomly reassigned into new pre- and post-pandemic groups, maintaining the same sample sizes as the original datasets. The MMD statistic is then recomputed for each permutation, generating an empirical distribution under the null hypothesis. The p-value is estimated as the proportion of permuted MMD values that exceed or equal the observed MMD, ensuring statistical robustness in assessing distributional differences.

The empirical p-value is calculated as:

\[
p = \frac{\sum_{b=1}^{B} \mathbb{1}(\text{MMD}^2_b \geq \text{MMD}^2_{\text{obs}})}{B}
\]

where \( B \) is the number of permutations, \( \text{MMD}^2_{\text{obs}} \) is the observed MMD statistic, and \( \mathbb{1}(\cdot) \) is an indicator function.

This methodology allows us to determine whether the observed shift in traffic patterns is statistically significant or could have arisen due to random fluctuations.

\section{Results}

\subsection{Generalized Additive Model (GAM)}

\subsection{Maximum Mean Discrepancy (MMD)}

To evaluate the statistical significance of traffic pattern shifts, we applied the Maximum Mean Discrepancy (MMD) test along with a permutation test. The results are visualized in heatmaps, where each cell represents the p-value associated with traffic distribution changes for a specific combination of day type and month group.

\subsubsection{Key Observations and Implications}

The findings indicate that the most notable adjustments in traffic patterns occur midweek (Tuesday–Thursday), where consistent shifts are observed across multiple month groups. Mondays and Fridays display a more variable pattern, with changes appearing more context-dependent. Weekend traffic remains relatively stable, with some month-specific variations.

These observations suggest that changes in travel behavior post-pandemic are more evident in structured weekday commuting patterns rather than in discretionary weekend travel. Additionally, differences between I-5 and I-205 may be influenced by location-specific factors such as infrastructure, urbanization, or regional travel demand.

\subsubsection{Summary of Observations}

Based on the analysis, the following key patterns emerge:

\begin{enumerate}
    \item Tuesday–Thursday traffic shows consistent changes across all month groups, suggesting adjustments in midweek commuting behavior.
    \item Monday and Friday display variability in changes, with certain months exhibiting stability and others showing shifts.
    \item Weekend travel remains largely stable, particularly on I-5, while I-205 exhibits some month-dependent variations.
    \item I-5 demonstrates broader consistency in traffic changes, while I-205 presents greater fluctuations across different month groups.
\end{enumerate}

These findings highlight how post-pandemic travel behavior varies across different day types, with midweek commuting experiencing the most pronounced shifts, while weekend travel remains relatively stable.

\section{Conclusions}

\appendix

\section{Median Peak Time and Volumes}

\definecolor{Gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{Gray}}r}
\begin{figure}[H]
\captionof{table}{Weekday Peak Time Table} \label{tab:medians} 
\begin{tabular}{l l *{9}{g}}
    \toprule
    \rowcolor{white}
    \multicolumn{3}{c}{\textbf{Location Info}} & 
    \multicolumn{4}{c}{\textbf{Median Peak Time}} & 
    \multicolumn{4}{c}{\textbf{Median Peak Volume}}\\

    \cmidrule(lr){1-3} \cmidrule(lr){4-7} \cmidrule(lr){8-11}

    \rowcolor{white}
    \textbf{ID} & \textbf{Name} & \textbf{Dir} & 
    \textbf{2018} & \textbf{2019} & \textbf{2023} & \textbf{2024} &
    \textbf{2018} & \textbf{2019} & \textbf{2023} & \textbf{2024}\\ 

    \cmidrule(lr){1-3} \cmidrule(lr){4-7} \cmidrule(lr){8-11}

    \input{median_table.txt}
\end{tabular}\par 
\bigskip 
    Hourly median traffic volume for each entry (location, direction, year) for weekdays is interpolated with a cubic spline to find approximate peak traffic time.
\end{figure}

\section{GAM Figures}

\input{gam_figures.txt}

\section{Data Availability}

\begin{figure}[H] 
    \centering
    \includegraphics[width=0.9\textwidth]{missing_data.png}
    \caption{Red indicates missing data, opacity indicates how much of that week is missing. This doesn't account for ``0'' data which is technically missing and misrepresented.}
    \label{fig:data_availability}
\end{figure}

\section{Data Issues Deep Dive by Locations} \label{app:data_issues}

These plots provide a visual for similar data presented in table \ref{tab:medians} but includes every year available in the dataset with quantile shading. Each line is the periodic cubic spline interpolation of the hourly medians/quantiles for each (location, direction, weekday/weekend) tuple. Some strange data issues become apparent in these plots, but they are almost entirely only in years we aren't concerned with in this analysis.
\input{quantile_figures.txt}

%\bibliography{references}
\end{document}

